# VerySmallLanguageModel

Educational transformer built from scratch using NumPy.

## Current milestone
- Single transformer block
- Forward pass only
- Causal self-attention
- LayerNorm + residuals
- No training yet

## Shapes
- Input: (B, T)
- Embedding: (B, T, D)
- Attention weights: (B, T, T)

## Next steps
- Add training loop
- Visualize attention evolution
