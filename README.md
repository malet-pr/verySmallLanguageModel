# VerySmallLanguageModel

Educational transformer built from scratch using NumPy.

## Current milestone
- Single transformer block
- Manual implementation of Gradient Descent and Backpropagation through time (BPTT) using NumPy.
- training loop
- tested with a java class

## Shapes
- Input: (B, T)
- Embedding: (B, T, D)
- Attention weights: (B, T, T)

## Next steps
- Modify model parameters to get a better prediction
- Visualize attention evolution
- Store the model
